{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jhaha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import ssl\n",
    "import os\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"CUAD_v1/full_contract_txt\"\n",
    "corpus = \"\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            corpus += file.read().lower() + \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = treebank_tokenizer.tokenize(corpus, convert_parentheses=True)\n",
    "\n",
    "with open('output_a.txt', 'w') as file:\n",
    "    for token in tokens:\n",
    "        file.write(f\"{token}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(tokens)\n",
    "\n",
    "sorted_tokens = token_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus: 4637915\n",
      "Number of unique tokens in corpus: 52715\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in corpus:\", len(tokens))\n",
    "print(\"Number of unique tokens in corpus:\", len(sorted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Token Ratio: 0.01136609877498833\n"
     ]
    }
   ],
   "source": [
    "type_token_ratio = len(sorted_tokens)/len(tokens)\n",
    "\n",
    "print(\"Type Token Ratio:\", type_token_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tokens = 0\n",
    "\n",
    "with open('tokens_c.txt', 'w') as file:\n",
    "    for token, freq in sorted_tokens:\n",
    "        if freq == 1:\n",
    "            single_tokens+=1\n",
    "        file.write(f\"{token}: {freq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens that appear only once: 22545\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens that appear only once:\", single_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_tokens = {'-LRB-', '-RRB-', '-LSB-', '-RSB-', '-LCB-', '-RCB-'}\n",
    "filtered_tokens = [token for token in tokens if token not in function_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens (Treebank): 4441204\n",
      "Final len(tokens) (no punctuation): 4107020\n"
     ]
    }
   ],
   "source": [
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "punctuation_tokens = regexp_tokenizer.tokenize(\" \".join(filtered_tokens))\n",
    "\n",
    "# Print results\n",
    "# print(punctuation_tokens)\n",
    "print(\"Original Tokens (Treebank):\", len(filtered_tokens))\n",
    "print(\"Final len(tokens) (no punctuation):\", len(punctuation_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_e.txt', 'w') as file:\n",
    "    for token in punctuation_tokens:\n",
    "        file.write(f\"{token}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_token_counts = Counter(punctuation_tokens)\n",
    "\n",
    "punctuation_sorted_tokens = punctuation_token_counts.most_common()\n",
    "\n",
    "with open('tokens_e.txt', 'w') as file:\n",
    "    for token, freq in punctuation_sorted_tokens:\n",
    "        file.write(f\"{token}: {freq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus without punctuation: 4107020\n",
      "Number of unique tokens in corpus without punctuation: 32071\n",
      "Type Token Ratio without punctuation: 0.0078088248900662766\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in corpus without punctuation:\", len(punctuation_tokens))\n",
    "print(\"Number of unique tokens in corpus without punctuation:\", len(punctuation_sorted_tokens))\n",
    "\n",
    "punctuation_type_token_ratio = len(punctuation_sorted_tokens)/len(punctuation_tokens)\n",
    "\n",
    "print(\"Type Token Ratio without punctuation:\", punctuation_type_token_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "no_stop_tokens = [word for word in punctuation_tokens if word.lower() not in stop_words]\n",
    "# print(no_stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_f.txt', 'w') as file:\n",
    "    for token in no_stop_tokens:\n",
    "        file.write(f\"{token}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_token_counts = Counter(no_stop_tokens)\n",
    "\n",
    "no_stop_sorted_tokens = no_stop_token_counts.most_common()\n",
    "\n",
    "with open('tokens_f.txt', 'w') as file:\n",
    "    for token, freq in no_stop_sorted_tokens:\n",
    "        file.write(f\"{token}: {freq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus without punctuation and stop words: 2405047\n",
      "Number of unique tokens in corpus without punctuation and stop words: 31937\n",
      "Type Token Ratio without punctuation and stop words: 0.013279158369878011\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in corpus without punctuation and stop words:\", len(no_stop_tokens))\n",
    "print(\"Number of unique tokens in corpus without punctuation and stop words:\", len(no_stop_sorted_tokens))\n",
    "\n",
    "no_stop_type_token_ratio = len(no_stop_sorted_tokens)/len(no_stop_tokens)\n",
    "\n",
    "print(\"Type Token Ratio without punctuation and stop words:\", no_stop_type_token_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigrams\n",
    "bigrams = list(nltk.bigrams(punctuation_tokens))\n",
    "\n",
    "pure_bigrams = []\n",
    "\n",
    "for words in bigrams:\n",
    "    if words[0] not in stop_words and words[1] not in stop_words:\n",
    "        pure_bigrams.append(words)\n",
    "\n",
    "pure_bigram_counts = Counter(pure_bigrams)\n",
    "\n",
    "pure_bigram_counts_sorted= pure_bigram_counts.most_common()\n",
    "\n",
    "with open('tokens_g.txt','w') as file:\n",
    "    for token, freq in pure_bigram_counts_sorted:\n",
    "        file.write(f\"{token}: {freq}\\n\")\n",
    "\n",
    "# print(\"Filtered tokens:\", no_stop_tokens)\n",
    "# print(\"Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282059\n",
      "363159\n",
      "Type to Token ratio: 0.28326231476086516\n"
     ]
    }
   ],
   "source": [
    "print(len(pure_bigrams))\n",
    "print(len(pure_bigram_counts))\n",
    "print(\"Type to Token ratio:\", len(pure_bigram_counts)/len(pure_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
